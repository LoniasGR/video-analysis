{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eEveggvc1aGi"
   },
   "source": [
    "# Εκτίμηση κίνησης σε Βίντεο"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqKWsv3q6D4F"
   },
   "source": [
    "## Ανοιγμα και επισκόπηση βίντεο\n",
    "Διαχειριζόμαστε αρχείο βίντεο με OpenCV, με τον ίδιο τρόπο που διαχειριζόμαστε στατικές εικόνες (ή αρχεία εικόνων).\n",
    "\n",
    "Η λογική μας είναι per-frame (ανά καρέ). Ανοίγουμε αρχικά το αρχείο video με την μέθοδο `cv2.VideoCapture()`, η οποία δέχεται σαν όρισμα ένα από τα παρακάτω:\n",
    "\n",
    "\n",
    "*   path για το αρχείο video\n",
    "*   device για capturing (id της κάμερας)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYts15w33F3p"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# video.mp4: input video\n",
    "cap = cv2.VideoCapture(\"Part2/Team30.mp4\")\n",
    "\n",
    "while (cap.isOpened()):\n",
    "  ret, frame = cap.read()\n",
    "\n",
    "  cv2.imshow(\"A simple video player\", frame)\n",
    "\n",
    "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "      break\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FxmU0vql4LwN"
   },
   "source": [
    "Στον παραπάνω κώδικα, η συνάρτηση `read()` της κλάσης VideoCapture επιστρέφει το επόμενο frame, όσο υπάρχουν frames στο αρχείο ή στο stream. Η `ret` αποθηκεύει μια bool τιμή σχετική με το αποτέλεσμα.\n",
    "\n",
    "Συμπεριφερόμαστε στο frame σαν σε μια κανονική εικόνα. Μπορούμε να την επεξεργαστούμε με OpenCV, numpy, scikit-image κλπ.\n",
    "\n",
    "Η `cv2.waitKey()` περιμένει τον αριθμό των milliseconds που θα της δώσουμε, ή μέχρι να πατήσουμε το πλήκτρο που ορίζουμε στο `ord()` για να προχωρήσει στο επόμενο καρέ ή να κόψει το while-loop αντίστοιχα."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8cvWWnnI6Ant"
   },
   "source": [
    "## Video properties\n",
    "\n",
    "Με την `get()` της κλάσης VideoCapture, μπορούμε να πάρουμε πληροφορίες για το βίντεο, χρήσιμες για τη συνέχεια. Η μέθοδος αυτή δέχεται σαν όρισμα έναν ακέραιο ή ένα flag της OpenCV, που υποδηλώνει το property που θέλουμε να διαβάσουμε:\n",
    "\n",
    "*   **cv2.CAP_PROP_FPS**: μας επιστρέφει τα FPS του βίντεο\n",
    "*   **cv2.CAP_PROP_FRAME_WIDTH** και **cv2.CAP_PROP_FRAME_HEIGHT**: μας επιστρέφει το width και το height (columns και rows) του κάθε καρέ. Εναλλακτικά μπορούμε να χρησιμοποιήσουμε τη `frame.shape`.\n",
    "\n",
    "Μπορείτε να βρείτε μια εξαντλητική λίστα των properties [εδώ](https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get). Στις τελευταίες εκδόσεις της python OpenCV, αφαιρείτε το αρχικό \"CV_\" από τα ονόματα των macros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "_9WFuVwp7q0y",
    "outputId": "4479669b-ae99-4832-ffdb-0236b7a17294"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "30.0\n1280.0\n720.0\n(720, 1280, 3)\n"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"../Assignment1/Part2/Team30.mp4\")\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "ret, frame = cap.read()\n",
    "\n",
    "print(frame.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bze1kpsQ9alG"
   },
   "source": [
    "# Optical flow\n",
    "Μέχρι στιγμής, έχουμε δει εφαρμογές που επεξεργάζονται μια εικόνα χωρικά, στο ίδιο frame. Τις ίδιες εφαρμογές μπορούμε να τις επεκτείνουμε και στο video.\n",
    "\n",
    "Βέβαια, το video περιλαμβάνει και χρονική πληροφορίά. Δεν πρόκειται απλά για μια αλληλουχία άσχετων εικόνων. Έτσι, χρειαζόμαστε τρόπους για να ανιγνεύσουμε αυτή τη σχέση ανάμεσα στα frames.\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=16_hfIQ5fr3S74Nz312UlufB0x4yfGs6f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g_T99iKh-x02"
   },
   "source": [
    "### Tι είναι το Optical flow\n",
    "To πρόβλημα του optical flow μπορεί να οριστεί ως εξής:\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1g0AKcxrxieA76espjs7ZeofFQjSfcxqZ\" width=\"70%\">\n",
    "\n",
    "όπου, ανάμεσα σε διαδοχικά frames, εκφράζουμε την τιμή φωτεινότητας (I) συναρτήσει του χώρου (x,y) και του χρόνου (t).\n",
    "$$I(x,y,t)=I(x+δx,y+δy,t+δt) $$\n",
    "\n",
    "που με προσέγγιση Taylor γίνεται: $I(x+δx,y+δy,t+δt)=I(x,y,t)+\\frac{\\partial(I)}{\\partial(x)}δx+\\frac{\\partial(I)}{\\partial(y)}δy+\\frac{\\partial(I)}{\\partial(t)}δt+...$ \n",
    "ή $\\frac{\\partial(I)}{\\partial(x)}u+\\frac{\\partial(I)}{\\partial(y)}v+\\frac{\\partial(I)}{\\partial(t)}=0$\n",
    "\n",
    "όπου $u=\\frac{dx}{dt}$ και $v=\\frac{dy}{dt}$, $\\frac{dI}{dx}$, $\\frac{dI}{dy}$ και $\\frac{dI}{dt}$ είναι οι παράγωγοι της εικόνας ως προς τον οριζόντιο άξονα, τον κάθετο άξονα και το χρόνο. \n",
    "\n",
    "\n",
    "Έτσι, το πρόβλημα του optical flow καταλήγει στο να βρούμε τις \"ταχύτητες\" $u (\\frac{dx}{dt})$ και $v (\\frac{dy}{dt})$  για να καθορίσουμε την κίνηση στο χρόνο."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pg10RqUPBzPU"
   },
   "source": [
    "### Εύρεση χαρακτηριστικών για tracking.\n",
    "\n",
    "Υπάρχουν δύο \"σχολές\" optical flow:\n",
    "\n",
    "\n",
    "*   **Sparse optical flow (Αραιή οπτική ροή):** Παρακολούθηση συγκεκριμένων χαρακτηριστικών ενός frame στην πορεία του βίντεο\n",
    "*   **Dense optical flow ((Πυκνή οπτική ροή):** Παρακολούθηση όλων των pixels του frame, στην πορεία του βίντεο\n",
    "\n",
    "Η δεύτερη είναι πιο ακριβής αλλά και πιο υπολογιστικά ακριβή."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fOwiEmkcB8K_"
   },
   "source": [
    "#### Sparse optical flow σε Shi-Tomasi features\n",
    "\n",
    "Για την εφαρμογή του optical flow, παρακολουθούμε μόνο την κίνηση ενός σετ χαρακτηριστικών. Tα features στις εικόνες είναι σημεία ενδιαφέροντος που παρουσιάζουν πλούσιες πληροφορίες περιεχομένου εικόνας. Για παράδειγμα, τέτοια features μπορεί να είναι σημεία στην εικόνα που είναι αμετάβλητα στη μετάφραση, την κλίμακα, την περιστροφή και τις αλλαγές έντασης όπως οι γωνίες.\n",
    "\n",
    "Ο Shi-Tomasi Corner Detector είναι πολύ παρόμοιος με τον γνωστό Harris Corner Detector. Ακολουθεί την παρακάτω διαδικασία:\n",
    "\n",
    "*   Καθορισμός παραθύρων με μεγαλες κλίσεις (διακυμάνσεις στην ένταση της εικόνας) όταν υπάρχει μετατόπιση τόσο κατά την διεύθυνση του x και του y.\n",
    "\n",
    "*   Για κάθε παράθυρο, υπολογίστε το score R\n",
    "*   Ανάλογα με την τιμή του R, κάθε παράθυρο ταξινομείται ως επίπεδη περιοχή (δίχως διακυμάνσεις στην φωτεινότητα), άκρο ή γωνία.\n",
    "\n",
    "\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1Egv5GSlo4z8xR5S6_oHSrWC0DufFEvle)\n",
    "\n",
    "O Shi-Tomasi detector θεωρείται ότι βελτιώνει τον Harris detector καθώς προσφέρει μια πιο robust και uniform αναπαράσταση των σχημάτων.\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1gItLF4m0CujefFkusEwGmeNsT5kuq6Lw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S5jzx0N3Rrgq"
   },
   "source": [
    "### Ο αλγόριθμος Lucas-Kanade\n",
    "\n",
    "Ο Lucas και ο Kanade πρότειναν μια αποτελεσματική τεχνική για την εκτίμηση της κίνησης των ενδιαφερόντων χαρακτηριστικών συγκρίνοντας δύο διαδοχικά καρέ. Η μέθοδος Lucas-Kanade λειτουργεί με τις ακόλουθες παραδοχές:\n",
    "\n",
    "1. Δύο διαδοχικά καρέ χωρίζονται με μια μικρή αύξηση χρόνου (dt) έτσι ώστε τα αντικείμενα να μην εκτοπίζονται σημαντικά (με άλλα λόγια, η μέθοδος λειτουργεί καλύτερα με αντικείμενα με αργή κίνηση).\n",
    "2. Ένα πλαίσιο απεικονίζει μια «φυσική» σκηνή αντικειμένων με αποχρώσεις του γκρι που αλλάζουν ομαλά.\n",
    "\n",
    "Kάτω από αυτές τις υποθέσεις, μπορούμε να πάρουμε ένα μικρό παράθυρο 3x3 (γειτονιά) γύρω από τα χαρακτηριστικά που εντοπίστηκαν από τον Shi-Tomasi και να υποθέσουμε ότι και τα εννέα σημεία έχουν την ίδια κίνηση.\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1BiXITXWrPH-BWMN3bRSd_Iu0AtATt09y)\n",
    "\n",
    "Αυτό είναι ένα set από 9 εξισώσεις, όπως αυτή που είδαμε στην αρχή:\n",
    "\n",
    "$\\begin{matrix}\n",
    "  I_x(q_1)V_x+I_y(q_1)V_y=I_t(q_1)\\\\\n",
    "  I_x(q_2)V_x+I_y(q_2)V_y=I_t(q_2)\\\\\n",
    "  \\vdots\\\\\n",
    "  I_x(q_n)V_x+I_y(q_n)V_y=I_t(q_n)\\\\\n",
    " \\end{matrix}$\n",
    "\n",
    "\n",
    "Σε μορφή συστήματος, αυτό γίνεται:\n",
    "\n",
    "\n",
    "$A=\\begin{bmatrix}\n",
    "  I_x(q_1)&&I_y(q_1)\\\\\n",
    "  I_x(q_2)&&I_y(q_2)\\\\\n",
    "  \\vdots&&\\vdots\\\\\n",
    "  I_x(q_n)&&I_y(q_n)\\\\\n",
    " \\end{bmatrix}\\quad \\quad$   $ u=\\begin{bmatrix}V_x\\\\ V_y\\end{bmatrix}\\quad \\quad$   $ b=\\begin{bmatrix}-I_t(q_1)\\\\-I_t(q_2)\\\\ \\vdots \\\\-I_t(q_n)\\end{bmatrix}$\n",
    "\n",
    "Ενώ στην αρχή είχαμε δύο αγνώστους (u και v) και μια εξίσωση, τώρα έχουμε δύο αγνώστους και 9 εξισώσεις!\n",
    "\n",
    "Χρησιμοποιούμε least squares fitting για να λύσουμε το πρόβλημα του over-determination:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "  V_x\\\\\n",
    "  V_y\\\\\n",
    "\\end{bmatrix} \\quad = \n",
    "\\quad \\begin{bmatrix}\\sum_iI_x(q_i)^2 && \\sum_iI_x(q_i)I_y(q_i)\\\\\\sum_iI_y(q_i)I_x(q_i) && \\sum_iI_y(q_i)^2 \\end{bmatrix}\n",
    "\\quad \\begin{bmatrix}-\\sum_iI_x(q_i)I_t(q_i)\\\\-\\sum_iI_y(q_i)I_t(q_i)\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Με λίγα λόγια, εντοπίζουμε μερικά ενδιαφέροντα χαρακτηριστικά για παρακολούθηση και υπολογίζουμε επαναληπτικά τα διανύσματα optical flow αυτών των σημείων. Ωστόσο, η υιοθέτηση της μεθόδου Lucas-Kanade λειτουργεί μόνο για μικρές κινήσεις (από την αρχική μας υπόθεση) και αποτυγχάνει όταν υπάρχει μεγάλη κίνηση.\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1tkw2Btbi7ZdZg_mMk4shSYvieb1coxip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "PCzF0cl8YYch",
    "outputId": "9b007f97-a75b-4857-c94e-1c76c72c862e"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "error",
     "evalue": "OpenCV(3.4.2) c:\\miniconda3\\conda-bld\\opencv-suite_1534379934306\\work\\modules\\imgproc\\src\\color.hpp:253: error: (-215:Assertion failed) VScn::contains(scn) && VDcn::contains(dcn) && VDepth::contains(depth) in function 'cv::CvtHelper<struct cv::Set<3,4,-1>,struct cv::Set<1,-1,-1>,struct cv::Set<0,2,5>,2>::CvtHelper'\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7e4b5c490b78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;31m# Calculates sparse optical flow by Lucas-Kanade method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.2) c:\\miniconda3\\conda-bld\\opencv-suite_1534379934306\\work\\modules\\imgproc\\src\\color.hpp:253: error: (-215:Assertion failed) VScn::contains(scn) && VDcn::contains(dcn) && VDepth::contains(depth) in function 'cv::CvtHelper<struct cv::Set<3,4,-1>,struct cv::Set<1,-1,-1>,struct cv::Set<0,2,5>,2>::CvtHelper'\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Parameters for Shi-Tomasi corner detection\n",
    "feature_params = dict(maxCorners = 300, qualityLevel = 0.2, minDistance = 2, blockSize = 7)\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow\n",
    "lk_params = dict(winSize = (15,15), maxLevel = 2, criteria = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "cap = cv.VideoCapture(\"../Assignment1/Part2/Team30.mp4\")\n",
    "\n",
    "color = (0, 255, 0)\n",
    "\n",
    "ret, first_frame = cap.read()\n",
    "\n",
    "# Converts frame to grayscale because we only need the luminance channel for detecting edges - less computationally expensive\n",
    "prev_gray = cv.cvtColor(first_frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "# Finds the strongest corners in the first frame by Shi-Tomasi method - we will track the optical flow for these corners\n",
    "prev = cv.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)\n",
    "\n",
    "# Creates an image filled with zero intensities with the same dimensions as the frame - for later drawing purposes\n",
    "mask = np.zeros_like(first_frame)\n",
    "\n",
    "while(cap.isOpened()):\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculates sparse optical flow by Lucas-Kanade method\n",
    "    next, status, error = cv.calcOpticalFlowPyrLK(prev_gray, gray, prev, None, **lk_params)\n",
    "\n",
    "    # Selects good feature points for previous position\n",
    "    good_old = prev[status == 1]\n",
    "\n",
    "    # Selects good feature points for next position\n",
    "    good_new = next[status == 1]\n",
    "\n",
    "    # Draws the optical flow tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        # a, b = coordinates of new point\n",
    "        a, b = new.ravel()\n",
    "\n",
    "        # a, b = coordinates of old point\n",
    "        c, d = old.ravel()\n",
    "\n",
    "        # Draws line between new and old position with green color and 2 thickness\n",
    "        mask = cv.line(mask, (a, b), (c, d), color, 2)\n",
    "\n",
    "        # Draws filled circle (thickness of -1) at new position with green color and radius of 3\n",
    "        frame = cv.circle(frame, (a, b), 3, color, -1)\n",
    "\n",
    "    # Overlays the optical flow tracks on the original frame\n",
    "    output = cv.add(frame, mask)\n",
    "\n",
    "    # Updates previous frame\n",
    "    prev_gray = gray.copy()\n",
    "\n",
    "    # Updates previous good feature points\n",
    "    prev = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    # Opens a new window and displays the output frame\n",
    "    #plt.imshow(\"sparse optical flow\", output)\n",
    "\n",
    "    # Frames are read by intervals of 10 milliseconds. The programs breaks out of the while loop when the user presses the 'q' key\n",
    "    if cv.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# The following frees up resources and closes all windows\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lab6_Motion_estimation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}